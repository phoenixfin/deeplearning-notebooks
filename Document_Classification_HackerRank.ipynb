{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document Classification - HackerRank",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNePqvVrSyG5BwdW0SEAP1D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phoenixfin/deeplearning-notebooks/blob/main/Document_Classification_HackerRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtdGTcZZqK8y"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CH7phbSm4XA",
        "outputId": "a3c28df1-3ef0-4b2f-bdfc-a2db80d24e39"
      },
      "source": [
        "!wget https://s3.amazonaws.com/hr-testcases/597/assets/trainingdata.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-15 07:18:10--  https://s3.amazonaws.com/hr-testcases/597/assets/trainingdata.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.110.254\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.110.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3337441 (3.2M) [text/plain]\n",
            "Saving to: ‘trainingdata.txt’\n",
            "\n",
            "trainingdata.txt    100%[===================>]   3.18M  5.29MB/s    in 0.6s    \n",
            "\n",
            "2020-12-15 07:18:11 (5.29 MB/s) - ‘trainingdata.txt’ saved [3337441/3337441]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDYOfrEIoAtn"
      },
      "source": [
        "data = []\n",
        "with open('/content/trainingdata.txt') as f:\n",
        "  for line in f.readlines()[1:]:\n",
        "    label = int(line.split(' ')[0])\n",
        "    sentence = line[2:]\n",
        "    data.append((sentence, label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnceHkDG3w0g"
      },
      "source": [
        "### Menyiapkan semua hiperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65VRmpmq3zxw"
      },
      "source": [
        "Berikut adalah semua hiperparameter yang akan digunakan di model. Semuanya akan berpengaruh pada performa training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yBsmXYFmz3S"
      },
      "source": [
        "batch_size = 32\n",
        "split = 0.2\n",
        "seed = 12\n",
        "vocab_size = 1000\n",
        "embedding_dim = 64\n",
        "max_length = 1000\n",
        "num_epochs = 100\n",
        "stopping_patience = 10\n",
        "success_threshold = 0.9\n",
        "learning_rate = 0.0005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CAcuoFr4Pon"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQa6VJ024eUZ"
      },
      "source": [
        "Memuat dataset dari direktori menggunakan metode\n",
        "[`text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory) sehingga diperoleh 2 objek `BatchDataset` dari tensorflow untuk training dan validasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0vlYZqod39-"
      },
      "source": [
        "def split_data(data):\n",
        "    # Separate out the sentences and labels into training and test sets\n",
        "    shuffled_data = random.shuffle(data)\n",
        "    training_size = int(len(data) * (1-split))\n",
        "\n",
        "    train_data = data[:training_size]\n",
        "    val_data = data[training_size:]\n",
        "    return train_data, val_data \n",
        "\n",
        "train_data, val_data = split_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9vBHihxZfCm"
      },
      "source": [
        "Memeriksa isi dari dataset yang telah digenerate dengan beberapa sampel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocnId4AV5y0d"
      },
      "source": [
        "Menyiapkan proses tokenisasi menggunakan layer [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) yang telah disediakan tensorflow. Data text yang ada dikonversi ke data integer. Kamus tokenisasi dibangun dari `train_ds`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg7a9sgQkabH"
      },
      "source": [
        "vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_sequence_length=max_length\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhVcd8ECoP-L"
      },
      "source": [
        "train_text = list(zip(*train_data))[0]\n",
        "vectorizer.adapt(train_text)\n",
        "def tokenize(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorizer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MogXOTrYaAt8"
      },
      "source": [
        "Menerapkan tokenisasi pada seluruh dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Uc1GCdolTp"
      },
      "source": [
        "def prepare_dataset(data_raw):\n",
        "    data_raw = list(zip(*data_raw))\n",
        "    sentences = tf.data.Dataset.from_tensor_slices(list(data_raw[0]))\n",
        "    one_hot_labels = tf.one_hot(list(data_raw[1]), 8)\n",
        "    labels = tf.data.Dataset.from_tensor_slices(one_hot_labels)\n",
        "    dataset = tf.data.Dataset.zip((sentences, labels))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(tokenize)\n",
        "    dataset = dataset.cache()\n",
        "    return dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_ds = prepare_dataset(train_data)\n",
        "val_ds = prepare_dataset(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjzl6NG1aIal"
      },
      "source": [
        "Periksa hasil tokenisasi dengan sampel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJtNLaZcaOwQ",
        "outputId": "140836ee-76f3-4725-9198-a2218f0c06fd"
      },
      "source": [
        "for token_batch, labels in train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(\"Tokens: \", token_batch.numpy()[i][:100], '...')\n",
        "        print(\"Labels: \", labels[i])\n",
        "        print('----------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens:  [  1 158  28   1  21 236  17  32  11  10  11  17  10  69   9  10   9  99\n",
            " 100  10  73  91  66   1   1 339   2 153   5  92  45  25   1 847  64 863\n",
            " 226   2  26   1 328 235  43 566 643   3   9  45  99 100   1   2  45 265\n",
            "   4 328   8   1  68 185 355   2   1 271  16   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0] ...\n",
            "Labels:  tf.Tensor([0. 1. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)\n",
            "----------------\n",
            "Tokens:  [303   1   1 787 303   1  66   1 429   4 749  19 787  63   1   5   6   1\n",
            "   4 614   1   5   1   8   1 449  13  27 127   1   1 102 338 170   1   1\n",
            "   7  14  12   1 246   1 770   6 787   1   7   5  43 865   2 338   1  57\n",
            " 794  19   2 921   1   8 209  12 852   8 261   1 854 218   1 306   5 184\n",
            "   8   6   1  35  84   1   4   1   4 395   1   2 737   2 148 854   1  33\n",
            "   2 158   3  71   2   1   1  66  60   1] ...\n",
            "Labels:  tf.Tensor([0. 0. 0. 0. 1. 0. 0. 0.], shape=(8,), dtype=float32)\n",
            "----------------\n",
            "Tokens:  [ 75   1   1   4 138   1  51  75   1  38   7  14 196   4 202   1   1  28\n",
            "  13  57   9  11   5 122  18  75 112   3   6   1 203  51  75   1   7   1\n",
            "  24   2 404  26  13  62 143  30 139 179   3   9  11  75   1  48  58 179\n",
            "  33   1   3  39  11   7   2   1 233 120  24  57 994   2  91   3   2  51\n",
            "  14   7   2  62   1 143  61   6 139   3 103   1   5   2   1 227   1 836\n",
            "  16   0   0   0   0   0   0   0   0   0] ...\n",
            "Labels:  tf.Tensor([0. 0. 1. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)\n",
            "----------------\n",
            "Tokens:  [  1 280  65   1 227  50 236  32  15  10  15  17  10  49   9  10   9  73\n",
            "  40  93 372 759  16   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0] ...\n",
            "Labels:  tf.Tensor([0. 1. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)\n",
            "----------------\n",
            "Tokens:  [  1  98 160 795 304  51 479  70   1  76   1   1   1  41  12 137   1   1\n",
            "   1   1 329   1   1   1   1   1   1   1   1   1 374   1   1   1  85   1\n",
            "   1   1   1   1   1   1   1   1   1   1   1  53   1   1   1   1   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0] ...\n",
            "Labels:  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)\n",
            "----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDvECDbp6XXV"
      },
      "source": [
        "### Setup fungsi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maeOUOdu6ghl"
      },
      "source": [
        "Akan didefinisikan beberapa fungsi yang akan dibutuhkan kelak:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elCPLp8l6j4G"
      },
      "source": [
        "#### Fungsi `plot_graphs` untuk menggambar kurva akurasi dan loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0F28Vj_rgOu"
      },
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_'+metric])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, 'val_'+metric])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "marPBSH_6qxp"
      },
      "source": [
        "#### Fungsi `set_callbacks` untuk mengatur callbacks yang akan dipakai di training\n",
        "\n",
        "Callbacks yang digunakan di sini ada 2, yakni untuk menghentikan proses training jika tidak ada progres signifikan dan untuk menghentikan proses training jika sudah mencapai akurasi yang diinginkan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sur-mNKORKtp"
      },
      "source": [
        "def set_callbacks(model):\n",
        "    callbacks = []\n",
        "    CB = tf.keras.callbacks\n",
        "\n",
        "    # no progress stopping callback\n",
        "    impatient = CB.EarlyStopping(\n",
        "        monitor='accuracy',\n",
        "        patience = stopping_patience)\n",
        "    callbacks.append(impatient)\n",
        "\n",
        "    # stop when enough callback\n",
        "    def stopper(epoch, logs):\n",
        "        if logs['accuracy'] > success_threshold and logs['val_accuracy'] > success_threshold: \n",
        "            model.stop_training = True\n",
        "    good_res = CB.LambdaCallback(on_epoch_end=lambda e,l: stopper(e,l))\n",
        "    callbacks.append(good_res)\n",
        "                        \n",
        "    return callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdznsqkp7Kny"
      },
      "source": [
        "#### Fungsi `build_model` untuk membangun model ML yang akan digunakan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2SngPpp7Rhn"
      },
      "source": [
        "Model yang dibangun adalah model Sequential dengan *Embedding* dan dua lapis *LSTM* dua-arah. Model ditutup dengan layer terkoneksi penuh berisi 5 neuron yang mewakili 5 kelas output dari teks yang diinput. Model selanjutnya dicompile dengan Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJY1TJ6cphXo"
      },
      "source": [
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero=True),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "        tf.keras.layers.Dense(8, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss = 'categorical_crossentropy',\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate),\n",
        "        metrics = ['accuracy']\n",
        "    )\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt0djeOD79qs"
      },
      "source": [
        "## Melatih Model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKTp0NOA8vzR"
      },
      "source": [
        "Nah ini saatnya membungkus semua yang sudah disiapkan.\n",
        "Langsung saja jalankan blok kode di bawah ini untuk melatih model dan langsung menggambar grafik akurasi dan loss-nya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r0AzHRCOq1oo",
        "outputId": "072a9603-8b07-46b8-9d72-430292992eb1"
      },
      "source": [
        "model = build_model()\n",
        "history = model.fit(\n",
        "    train_ds, \n",
        "    epochs = num_epochs, \n",
        "    validation_data = val_ds, \n",
        "    callbacks = set_callbacks(model)\n",
        ")\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 64)          64000     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 1000, 128)         66048     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 229,896\n",
            "Trainable params: 229,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "138/138 [==============================] - 369s 3s/step - loss: 0.9321 - accuracy: 0.6522 - val_loss: 0.5296 - val_accuracy: 0.7922\n",
            "Epoch 2/100\n",
            "138/138 [==============================] - 364s 3s/step - loss: 0.4422 - accuracy: 0.8143 - val_loss: 0.3784 - val_accuracy: 0.8314\n",
            "Epoch 3/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.3434 - accuracy: 0.8535 - val_loss: 0.3277 - val_accuracy: 0.8396\n",
            "Epoch 4/100\n",
            "138/138 [==============================] - 364s 3s/step - loss: 0.3184 - accuracy: 0.8576 - val_loss: 0.3258 - val_accuracy: 0.8387\n",
            "Epoch 5/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.2594 - accuracy: 0.8756 - val_loss: 0.3015 - val_accuracy: 0.8532\n",
            "Epoch 6/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.2951 - accuracy: 0.8544 - val_loss: 0.3087 - val_accuracy: 0.8523\n",
            "Epoch 7/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.2286 - accuracy: 0.8856 - val_loss: 0.2880 - val_accuracy: 0.8587\n",
            "Epoch 8/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.2086 - accuracy: 0.8965 - val_loss: 0.2680 - val_accuracy: 0.8614\n",
            "Epoch 9/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.1976 - accuracy: 0.8988 - val_loss: 0.2691 - val_accuracy: 0.8614\n",
            "Epoch 10/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1788 - accuracy: 0.9050 - val_loss: 0.2619 - val_accuracy: 0.8687\n",
            "Epoch 11/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1712 - accuracy: 0.9068 - val_loss: 0.2773 - val_accuracy: 0.8733\n",
            "Epoch 12/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1772 - accuracy: 0.9045 - val_loss: 0.2680 - val_accuracy: 0.8715\n",
            "Epoch 13/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1978 - accuracy: 0.8984 - val_loss: 0.3138 - val_accuracy: 0.8596\n",
            "Epoch 14/100\n",
            "138/138 [==============================] - 367s 3s/step - loss: 0.1699 - accuracy: 0.9088 - val_loss: 0.2754 - val_accuracy: 0.8696\n",
            "Epoch 15/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1689 - accuracy: 0.9100 - val_loss: 0.2311 - val_accuracy: 0.8879\n",
            "Epoch 16/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1637 - accuracy: 0.9104 - val_loss: 0.2334 - val_accuracy: 0.8952\n",
            "Epoch 17/100\n",
            "138/138 [==============================] - 368s 3s/step - loss: 0.1393 - accuracy: 0.9168 - val_loss: 0.2196 - val_accuracy: 0.8924\n",
            "Epoch 18/100\n",
            "138/138 [==============================] - 367s 3s/step - loss: 0.1315 - accuracy: 0.9223 - val_loss: 0.2152 - val_accuracy: 0.8988\n",
            "Epoch 19/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1167 - accuracy: 0.9264 - val_loss: 0.2278 - val_accuracy: 0.8961\n",
            "Epoch 20/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1155 - accuracy: 0.9278 - val_loss: 0.2623 - val_accuracy: 0.8897\n",
            "Epoch 21/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1114 - accuracy: 0.9280 - val_loss: 0.2644 - val_accuracy: 0.8897\n",
            "Epoch 22/100\n",
            "138/138 [==============================] - 366s 3s/step - loss: 0.1180 - accuracy: 0.9266 - val_loss: 0.2623 - val_accuracy: 0.8961\n",
            "Epoch 23/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.1394 - accuracy: 0.9182 - val_loss: 0.2278 - val_accuracy: 0.8952\n",
            "Epoch 24/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.1292 - accuracy: 0.9273 - val_loss: 0.2446 - val_accuracy: 0.8833\n",
            "Epoch 25/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.1492 - accuracy: 0.9202 - val_loss: 0.2792 - val_accuracy: 0.8833\n",
            "Epoch 26/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.1255 - accuracy: 0.9262 - val_loss: 0.2424 - val_accuracy: 0.8906\n",
            "Epoch 27/100\n",
            "138/138 [==============================] - 365s 3s/step - loss: 0.1124 - accuracy: 0.9319 - val_loss: 0.2365 - val_accuracy: 0.9015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-87927d436911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-85e5c6debb55>\u001b[0m in \u001b[0;36mplot_graphs\u001b[0;34m(history, metric)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    }
  ]
}